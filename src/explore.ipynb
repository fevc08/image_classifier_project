{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Image classification system\n",
                "\n",
                "### The dataset is composed of dog and cat photos provided as a subset of photos from a much larger 3 million manually annotated photos. This data was obtained through a collaboration between Petfinder.com and Microsoft.\n",
                "\n",
                "### The data set was originally used as a CAPTCHA, i.e., a task that a human is believed to find trivial, but that a machine cannot solve, which is used on websites to distinguish between human users and bots. The task was named \"Asirra\". When \"Asirra\" was introduced, it was mentioned \"that user studies indicate that humans can solve it 99.6% of the time in less than 30 seconds.\" Barring a breakthrough in computer vision, we expect that computers will have no more than a 1/54,000 chance of solving it.\n",
                "\n",
                "### At the time the competition was published, the state-of-the-art result was achieved with an SVM and was described in a 2007 paper with the title \"Machine Learning Attacks against Asirra's CAPTCHA\" (PDF) that achieved 80% classification accuracy. It was this paper that showed that the task was no longer a suitable task for a CAPTCHA shortly after the task was proposed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the libraries\n",
                "import os\n",
                "import shutil\n",
                "import matplotlib as plt\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.utils import set_random_seed\n",
                "from tensorflow.keras.losses import BinaryCrossentropy\n",
                "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "from tensorflow.keras.models import load_model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Step 1:** Loading the dataset and visualize the input information"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the path to your dataset, where you have subdirectories for each class under train/ and test/\n",
                "train_dir_source = \"../data/raw/train/\"\n",
                "test_dir = \"../data/raw/test1/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset organized into subdirectories.\n"
                    ]
                }
            ],
            "source": [
                "# Define the destination directory where you want to organize the dataset\n",
                "train_dir = \"..//data//processed//train//\"\n",
                "\n",
                "# Create subdirectories for 'cats' and 'dogs' within the destination directory\n",
                "os.makedirs(os.path.join(train_dir, \"cats\"), exist_ok=True)\n",
                "os.makedirs(os.path.join(train_dir, \"dogs\"), exist_ok=True)\n",
                "\n",
                "# List all files in the source directory\n",
                "files = os.listdir(train_dir_source)\n",
                "\n",
                "# Iterate through the files and move them to the appropriate subdirectory\n",
                "for file in files:\n",
                "    if file.startswith(\"cat\"):\n",
                "        # Move cat images to the 'cats' subdirectory\n",
                "        shutil.move(os.path.join(train_dir_source, file), os.path.join(train_dir, \"cats\", file))\n",
                "    elif file.startswith(\"dog\"):\n",
                "        # Move dog images to the 'dogs' subdirectory\n",
                "        shutil.move(os.path.join(train_dir_source, file), os.path.join(train_dir, \"dogs\", file))\n",
                "\n",
                "print(\"Dataset organized into subdirectories.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 25000 images belonging to 2 classes.\n",
                        "Found 0 images belonging to 0 classes.\n"
                    ]
                }
            ],
            "source": [
                "# Define data augmentation and preprocessing for training data\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1.0 / 255,  # Rescale pixel values to [0, 1]\n",
                "    rotation_range=40,  # Randomly rotate images\n",
                "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
                "    height_shift_range=0.2,  # Randomly shift images vertically\n",
                "    shear_range=0.2,  # Shear transformations\n",
                "    zoom_range=0.2,  # Randomly zoom in on images\n",
                "    horizontal_flip=True,  # Randomly flip images horizontally\n",
                "    fill_mode='nearest'  # How to fill in newly created pixels after rotation or shifts\n",
                ")\n",
                "\n",
                "# Define preprocessing for the test data (only rescaling)\n",
                "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
                "\n",
                "# Set batch size\n",
                "batch_size = 32\n",
                "\n",
                "# Create generators for training and testing data\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    train_dir,\n",
                "    target_size=(200, 200),  # Resize images to 200x200 pixels\n",
                "    batch_size=batch_size,\n",
                "    class_mode= \"binary\"\n",
                ")\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    test_dir,\n",
                "    target_size=(200, 200),\n",
                "    batch_size=batch_size,\n",
                "    class_mode= \"binary\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Step 3:** Build an ANN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "set_random_seed(42)\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Conv2D(input_shape=(200, 200, 3), filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
                "model.add(Conv2D(filters = 64,kernel_size = (3,3),padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(units = 4096,activation = \"relu\"))\n",
                "model.add(Dense(units = 4096,activation = \"relu\"))\n",
                "model.add(Dense(units=1, activation=\"sigmoid\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile the model\n",
                "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/2\n",
                        "782/782 [==============================] - 26715s 34s/step - loss: 0.6969 - accuracy: 0.4923\n",
                        "Epoch 2/2\n",
                        "782/782 [==============================] - 26745s 34s/step - loss: 0.6932 - accuracy: 0.4983\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<keras.src.callbacks.History at 0x15c1a4a8a90>"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Fit the model using the data generator\n",
                "model.fit(train_generator, epochs = 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " 89/782 [==>...........................] - ETA: 1:41:09 - loss: 0.6932 - accuracy: 0.4989"
                    ]
                }
            ],
            "source": [
                "# Measure the performance of the model\n",
                "_, accuracy = model.evaluate(train_generator)\n",
                "print(f\"Accuracy: {accuracy}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Step 4:** Optimize the above model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/3\n",
                        " 26/782 [..............................] - ETA: 7:02:14 - loss: 0.6931 - accuracy: 0.5120"
                    ]
                }
            ],
            "source": [
                "# Create the object for ModelCheckpoint and EarlyStopping\n",
                "checkpoint = ModelCheckpoint(\"../models/binary_crossentropy.h5\", monitor = \"val_accuracy\", verbose = 1, save_best_only = True, save_weights_only = False, mode = \"auto\")\n",
                "early = EarlyStopping(monitor = \"val_accuracy\", patience = 3, verbose = 1, mode = \"auto\")\n",
                "\n",
                "# Create the object for callback functions to fit_generator\n",
                "history = model.fit(train_generator, epochs = 3, validation_data = test_generator, validation_steps = 10, callbacks = [checkpoint, early])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the performance\n",
                "plt.plot(history.history['val_loss'])\n",
                "plt.plot(history.history['loss'])\n",
                "plt.xlabel('Loss')\n",
                "plt.ylabel('Iterations')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the saved model\n",
                "model = load_model(\"../models/binary_crossentropy.h5\")\n",
                "\n",
                "# Evaluate the model using the test data generator\n",
                "test_loss, test_accuracy = model.evaluate(test_generator)\n",
                "print(f\"Test Accuracy: {test_accuracy}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
